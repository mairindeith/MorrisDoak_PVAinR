---
title: "Chapter 2 - The Causes and Quantification of Population Vulnerability"
author: "Mairin Deith"
output:
  html_document: default
  word_document: default
  pdf_document: default
---
<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{natserv vignette}
%\VignetteEncoding{UTF-8}
-->
=====

# Chapter 2 summary

Chapter 2 of Morris and Doak's *Quantitative Conservation Biology: Theory and Practice of Population Vulnerability Analysis* discusses how a population's vulnerability is influenced by factors like temporal variability (e.g. environmental stochasticity, demographic stochasticity, and catastrophic/bonanza events). 
All of these factors can negatively impact overall population growth by introducing variability in the population's vital rates. As a result of variability, populations can decrease in size even if the arithmetic mean of population growth indicates positive growth (i.e. $\lambda_A > 1$).

The effects of temporal variability are magnified as we project further forward in time. 

# Box 2.1 - Simulating population trajectories under temporal variability

This example model uses the basic equation of population growth, $N_{t+1} = \lambda_t N_t$ (Eq'n 2.1), but randomly draws the value of $\lambda_t$ from a set of observed values of $\lambda$. 
While Morris and Doak provide their code in MATLAB, I have translated it to a free and open-source statistical language, R, below - there are also comments to indicate which parts of the MATLAB code correspond to R commands. 
All variable names are kept the same from the original MATLAB code, only the functions and code structure have been modified.

```{r load libraries}
### PROGRAM RandDraw.m, translated to RandDraw.R
### Performs multiple simulations of discrete exponential growth trajectores using a set of observed lambda values

### Load R libraries
# This code only uses three optional libraries, ggplot2 and the tidyverse packages tidyr and dplyr
# These packages are only used to make the plots produced by population simulations, but are not necessary for the actual simulations. 
# If you'd rather not install and load these libraries, it won't affect the population simulation steps
library(ggplot2)
library(tidyverse)
```

```{r set up population parameters}
### SIMULATION PARAMETERS
# The observed lambda values to sample from. Each has equal draw probability.
lams <- c(1.00, 1.98, 1.02, 0.92, 0.53) 
# Original population size at t=0
numzero <- 29
# Number of years to simulate 
tmax <- 100
# How many replicates of the time series should be created
numreps <- 100
# We ignore 'outname' in R because our results will exist in our R session

### MODEL
# This replaces the rand(...) MATLAB command, sets the seed for the random number generator
# SN: Unlike MATLAB, R does not require setting the seed for randomization, but setting it to 123 will ensure that your results match the ones I generated here
set.seed(seed=123) 
numlambs <- length(lams) # How many lambda values are there?

# To generate a matrix of lambdas to use for each year/simulation:
#    a. Create a 100x100 matrix (one index for each value of 1:tmax and 1:numreps
#    b. For each value in the matrix, randomly sample a value of lambda (with replacement) from the lams list
# This combines the commands that create intvalues and lambdas in the original M+D MATLAB code
lambdas <- matrix(nrow=tmax, ncol=numreps, data=sample(lams, tmax*numreps, replace=T))
dim(lambdas) # Just to check if it matches our expectations of size and values
unique(as.numeric(lambdas))
```

```{r pop.model}
# Now we need to initialize population size
initpopsize <- rep(numzero, numreps) # initpopsize is a length-100 vector filled with 29, our starting population size

# For each year of the simulation, multiply the current population size by the lambdas*last year's population size
#   a) Initialize the popsizes matrix with NaN values - this will be populated later
#   b) For each year, calculate the new population size following deterministic growth with that year's lambda_t
# SN: There is likely a faster/more code-efficient way to do this; please raise an Issue on GitHub if you know a better solution
popsizes <- matrix(nrow=tmax, ncol=numreps, data=NaN)
popsizes[1,] <- initpopsize # The first row of our simulation (i.e. our first year) is 29 for all replications
# Now iterate over years to calculate the new population size
# If the previous year's population is less than one, enforce a new population size of 0 
#    This ensures that we don't see populations with less than 1 individual (e.g. 0.5) rebuilding the population
for(y in 2:tmax){
  lambdas.year <- lambdas[y-1,]
  # Temporary vector indicating whether the population is extinct in the past year or not
  notextinct.tmp <- (popsizes[y-1,]>=1)*1
  popsizes[y,] <- popsizes[y-1,]*lambdas.year*notextinct.tmp
}

# Let's check on how the first few years of simulation look in the first 10 replications
# after giving the matrix some better column and row names
colnames(popsizes) <- paste0('rep',1:numreps)
# rownames(popsizes) <- paste0('y',1:tmax)

head(popsizes[,1:10])
```

We can already see the impact that a variable $\lambda_t$ can have on population growth dynamics.
```{r}
# Now identify situations in which the population went extinct
notextinct <- (popsizes>=1)*1
table(notextinct)
```
In 1957 instances, we saw extinction. 
```{r}
# Find the biggest population size
maxN <- 0.5*max(popsizes)
maxN
```
Oh boy...that's a really big number. 
Let's set a cutoff of 200, like in the textbook. 
```{r}
# Calculate the geometric mean for lambda
stochL <- prod(lams)^(1/numlambs)
# Calculate the expected population at tmax
expectedNt <- numzero*(stochL^tmax)

cat('Stochastic lambda:', stochL); cat('\nExpected N at t=tmax:', expectedNt) 
```
Overall, we should expect the population to decrease over time (compared to the arithmetic mean, 1.09)
```{r}
# We're going to skip writing to the worksheet and plot the population sizes using ggplot
# Find the average population size for each year, averaging across replications
averageN <- data.frame(averagePop=rowMeans(popsizes[,1:100]),
                       year=1:tmax)

# Let's get to the histogram in a moment, first let's see a time series
#    To do this, I'm going to first rearrange the dataframe using tidyr
popsizes.mod <- as.data.frame(popsizes) %>% 
  gather(key = rep, value = populationSize) %>% 
  mutate(year = rep(1:tmax,numreps))
  # I need to add the +1 here to account for the annual mean values

ggplot(data=as.data.frame(popsizes.mod), aes(x=year, y=populationSize, color=rep)) + 
  geom_line(alpha=0.7) + # plot out our replicated series
  geom_line(data=averageN, aes(y=averagePop, x=year), color='black', size=1.2) + # Plot the average size overtop
  ylim(0,2000) + # limit our y axis from 0 to 2000
  theme(legend.position = 'none') # get rid of the legend
```
Now let's make those histograms after bounding our data to be below 2000 individuals (like the y-axis above).
```{r}
bounded.popsizes <- popsizes.mod
bounded.popsizes$populationSize[which(bounded.popsizes$populationSize>2000)] <- 2000

# Let's check that worked:
summary(bounded.popsizes)
# Good, the max value is 2000.
```

```{r}
# Histogram for the first 5 years
ggplot(data=bounded.popsizes[which(bounded.popsizes$year %in% c(1:5)),], aes(x=populationSize)) +
  geom_histogram()
```
```{r}
# For the first 20 years
ggplot(data=bounded.popsizes[which(bounded.popsizes$year %in% c(1:20)),], aes(x=populationSize)) +
  geom_histogram()
```
```{r}
# For the full 100 years
ggplot(data=bounded.popsizes, aes(x=populationSize)) +
  geom_histogram()
```

That's the end of the practical example in M+D's PVA textbook.
The remainder of the chapter discusses the influence of negative density dependence, positive density dependence (i.e. Allee effects), and genetic factors on population growth and viability. 

# Notes for the remainder of the chapter
## Density dependence
### Negative density dependence 
* **Density dependence** = a change in individual performance as the size or density of the population changes; embodied by the logistic growth equation  $N_{t+1} = N_t + rN_t(1-\frac{N_t}{K})$
# CHECK THE ABOVE EQUATION

* can cause PVA to over or underestimate the true viability of a population depending on how growth rates respond to density
* relationship is typically nonlinear - effects may not manifest until high population densities
* if we assume linear negative density dependence - greater probability of species extinction; this is because **negative density dependence maintains the population at a low level** because higher densities = lower fecundity
   * at the same time, because fecundity is higher at low densitites, the risk of extinction **can also decrease the face of negative density dependence** as a result of reproductive compensation
   
In contrast, if density dependence is introduced only at a high densities, there will be **no added protection of density dependence as fecundity is not increased at low densities**. 
If PVA assumed perfect density dependence, this would artificially increase our confidence that the population will persist.

As a result, many PVAs ignore density dependence although many methods exist to estimate and include negative density dependence in PVA (or use a simple population ceiling). 
It is usually best to include no density dependent effects if careful testing of population data indicate no density dependence *or* create a range of models with different strengths/forms of density dependence. 

### Positive density effects/Allee effects
* **increase in population growth rate as size increases** - arise from improved mating success, group defense\*, or group foraging\* as density increases (majority of evidence from sessile plants)

\* - only important for some species, while all affected by mate success

* **+ DD only rarely detected with census data** - e.g. using quadratic regression between per-capita population growth rate and N in birds/mammals with census data @ low sizes - expected to see a hump-shaped curve
* neither analysis found any evidence for + DD
* Myers et al. looked for + DD in fish - fit BH models with and without allee effects - in only 3/128 stocks did allee effects fit the recruitment data better than BH without
* logistic regression of butterflies showed proportion of small populations that grew or remained constant from *t* to *t+1* was positively related to population size at *t* - looked at replication of multiple populations in space rather than a single population over time
* **thus we do not have a good idea of how Allee effects operate or how strong they are, even though they are almost certainly operating**
   * we can incorporate these into PVas, but the limits of our data can thwart inclusion of positive density effects; best solution often to set a quasi-extinction threshold high enough to avoid Allee effects 
   * But what this level is can be confusing (e.g. passenger pigeons may have been 10,000; but for some plants it is more like 20) - again, best to run a set of PVA models and compare their outcomes
   
## Genetic factors

At low population sizes, genetic factors are especially important. 
Inclusion in PVA requires **estimates of 2 sets of processes**:

1. Rate at which heterozygosity (genetic diversity, *probability that for the average locus in the average individual in the population, there will be different alleles*) will be lost by a pop'n of certain size
   * This is often estimated as the change in **inbreeding level per generation** (*inbreeding = average probability that an individuals two copies of a gene are 'identical by descent'*)
2. What are the consequences of how fast the loss of heterozygosity will be? 
   * decrease in individual performance as a result of inbreeding = **inbreeding depression**, commonly measured as a percent change in some vital rate (e.g. survival, reproduction) with a loss in heterozygosity/increase in inbreeding level

Because it can be very difficult to estimate the influence/extent of breeding depressions on wild species (domesticated easier), some argue that if pop sizes are low enough to suffer from inbreeding depressions, they are susceptible to demographic stochasticity anyway. *But some studies show synergistic effects with demography to impact population health*. 

This feeds the **extinction vortex**:

1. e.g. Environmental variation = low levels for one generation
2. = small increase in inbreeding
3. = slightly decreased vital rates compared to what it was before
4. If the population drops again, it will stay there longer due to poorer per-capita performance
5. = increased susceptability to env and dem stochasticity

While **purging** can eliminate deleterious alleles through natural selection and reduce the severity of subsequent bottlenecks. *But the rate of purging of deleterious alleles depends on the rate of inbreeding and the genetic mechanism that creates the inbreeding depression*. 

Generally, do not include genetic effects in PVA - again, use a quasi-extinction threshold (e.g. $N_e = 50$, effective population size > 50 individuals tends to result in only 1% loss of genetic diversity per generation). 
$Ne \approx \frac{N}{2} or \frac{2N}{3}$ where $N$ is the number of **reproductive adults**. 

Still basically ignores inbreeding problems, but it's better to not make wild guesses.

# Quantifying population viability

## Viability metrics related to Pr(quasi-extinction)

**No good PVA should evaluate the risk of total extinction** - the process is complicated, and we shouldn't assume to know the true population dynamics that might influence ultimate extinction. 
Therefore it's more appropriate to measure **quasi-extinction thresholds** - the number of females that should be considered a bare minimum - below this level, the population is likely to be critically and immediately imperiled. 

Based on dem. stoch, this could be ~20 reproductive individuals; genetic arguments favour >100 breeders minimum. 
*Lower values may be required if the population is already below some preferable extinction threshold - e.g. African elephant's QE-threshold is 1*.

```{r}
plot(dgamma(x=seq(0,100),shape=2, scale=8),type = 'l', ylab='Probability density function',xlab='Years into the future', main='Example extinction risk')
abline(v = which(dgamma(x=seq(0,100),shape=2, scale=8) == max(dgamma(x=seq(0,100),shape=2, scale=8))), lty=2)
```
Many PVAs show this relationship; extinction is most likely at the beginning - if it hasn't gone extinct in the first 10 years, there is less likelihood that it will eventually go extinct. 

```{r}
plot(pgamma(q=seq(0,100),shape=2, scale=20),type = 'l', ylab='Cumulative distribution function',xlab='Years into the future', main='Example extinction risk')
abline(h=0.5, lty=2)
```

The cumulative distribution function is much more applicable for PVA - where we are interested what the probability of QE is *at or before some time*, not *at some time*. 
It asymptotes to 1 because at some point, the species will almost certainly go extinct. 

### Valuable data from the probability distributions of extinction risk

1. Probability of extinction *by some given point*
2. Ultimate probability of extinction = probability that the QE threshold will be reached *at any time*
3. Median time to extinction; **median** = point where there is a 50% cumulative probability (dashed line) 
4. Mean time to extinction (not obvious from probability plots, but can be calculated from the data)
5. Mode of extinction time = peak of the probability distribution (dashed line)

**1-3 are the most useful, 4-5 are the most commonly used but least useful**.
It is important to show probability of extinction within some management timescale - not just a biological decision, but funding deadlines, future threats, political changes, and other real-world constraints. 
But because the choice can be somewhat subjective, it is rarely used. Sad. 

Together, the median time to extinction and the ultimate risk of extinction are meaningful. 
Median = good description of extinction time (half of the possible paths lead to extinction by this time), ultimate is needed to show just how likely extinction at any time is. 
If the probability of extinction ever happening is low, so is the median (this can happen if a population is quite small and expected to grow, and if extinctions occur they will occur early). 

Mean time is almost always an overestimate, and the mode doesn't look at cumulative distributions - considers only the instant at which extinction is most likely to occur instantaneously, not cumulatively. Mode is always shorter than median. 

Overall estimates of extinction risk aren't particularly useful - all species might eventually go extinct, but their extinction is over such a long timescale that it isn't practical. 

PVAs should **show the entire cumulative distribution of extinction times, discuss the risk of extinction by certain time horizons with biological/practical considerations**. The distribution is far more useful than any summary statistics (*accidental Bayesian*).

## Viability metrics related to population growth rate
**Always use an estimate of stochastic growth!**

Growth rate may be a more important indicator of possible future problems - e.g. there are ~150 female right whales, and the population is declining as of 1999 ($\lambda = 0.976$). It is likely to go extinct in 300 years, with near-zero risk over the next 100. 

Another benefit - easier to estimate reliably with spotty/short-term data. 

Ultimately, there is a massive argument about whether small populations are doomed as a result of environmental stochasticity (*small population paradigm*), or deterministic factors (*declining population paradigm*, e.g. Caughley 1994).
Obviously, stochasticity does matter **a lot**, so let's not ignore it. 

## Viability metrics related to population size and number 
Typically, some target population size/level is known to be 'safe'. 
This automatically includes some assumptions about growth (>1), probability of extinction is low once that size is achieved. 

However, the target sizes are typically decided somewhat ad-hoc by committees without data/sound science.
e.g. ESA - 28% of recovery plans set the population size for recovery **at or below the number of individuals in existance at the time of writing, 37% set the target at or below the number at time of listing**.
For plants, the goals correlated very strongly with those in existence at the time of writing. 

e.g. US Marine Mammal Protection Act (MMPA) - on the assumption that a carrying capacity exists for each population, protection is necessary at 60% of capacity when it is classified as **depleted** (typically more conservative than ESA listings). However, this is highly inflexible - if 2 species exist at the same numbres, but one was once much more common, they are treated as having different risk (which may not be the case). 

e.g. IUCN - adopted criteria for endangerment that rely on estimates of extinction risk, but these have been roughly translated into population numbers, sizes, and trends so they can be applied without exhaustive information. 

But the criteria are complex - developed to be used with a variety of kinds/qualities of data.
Critically endangered species = those which show either (a) Reductions of at least 80% in the last 10 years/3 generations (whichever is longer), (b) an extent of occurence of less than 100km2 or area of occupancy less than 10km2, (c) a population of less than 250 individuals with evidence of decline, (d) popopulation of less than 50 mature individuals, or (e) 50% probability of extinction in the wild in 10 years/3 generations. 

If there is no possibility for doing a PVA, the simple numerical criteria are useful. But larger populations can also be deeply imperiled - current population size is a 'second-best' option compared to PVA. They are useful for triage, but must be done with the knowledge that small stable populations may be at lower risk than larger populations subjected to strong variability. 

A better option = **perform an analysis of population dynamics** to assess future stochastic population growth rate and risk of extinction.